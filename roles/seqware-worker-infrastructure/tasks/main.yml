---
# file: roles/seqware-worker-infrastructure/tasks/main.yml

- name:  Install Hadoop for a worker
  apt: name={{ item }} state=present update_cache=yes cache_valid_time=3600 install_recommends=no
  with_items: 
    - hadoop-0.20-mapreduce-tasktracker 
    - hadoop-hdfs-datanode
    - hadoop-client
    - hbase-regionserver

# This code is duplicated and should be refactored to a shared task, but still run after the dependencies above
# Need to figure out how to share tasks as opposed to roles
- name:  Set user groups
  user: name=mapred groups=seqware append=yes state=present
  user: name=seqware groups=mapred append=yes state=present
  user: name=mapred groups=hadoop append=yes state=present

- name: Make sure directory exists
  file: path=/etc/hadoop/conf.my_cluster state=directory

- name:  Copy resources
  template: 
    src: hadoop-files/conf.my_cluster/{{ item }}
    dest: /etc/hadoop/conf.my_cluster/{{ item }}
  with_items:
      - capacity-scheduler.xml
      - hadoop-metrics2.properties  
      - log4j.properties            
      - slaves                  
      - yarn-site.xml
      - configuration.xsl       
      - hadoop-metrics.properties   
      - mapred-queues.xml.template  
      - ssl-client.xml.example
      - container-executor.cfg  
      - hadoop-policy.xml           
      - mapred-site.xml             
      - ssl-server.xml.example
      - core-site.xml           
      - hdfs-site.xml               
      - mapred-site.xml.template    
      - yarn-env.sh
  
- name:  Set up Hadoop configuration
  command: 'update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.my_cluster 50'
  when: hadoopconfig.changed

- name:  Set up Hadoop configuration
  command: 'update-alternatives --set hadoop-conf /etc/hadoop/conf.my_cluster'
  when: hadoopconfig.changed
  
- name:  Start HDFS
  service: name=hadoop-hdfs-datanode state=started
  
- name:  Start TaskTracker
  service: name=hadoop-0.20-mapreduce-tasktracker state=started

- name:  Setup daemons to start on boot
  service: name={{ item }} state=started enabled=yes
  with_items: 
    - cron
    - hadoop-hdfs-datanode
    - hadoop-0.20-mapreduce-tasktracker

- name:  Install NFS
  apt: name={{ item }} state=present update_cache=yes cache_valid_time=3600
  register: nfs_installed
  with_items: 
    - rpcbind
    - nfs-common
    
- name:  Setup NFS - hosts.deny
  lineinfile: 
    dest: /etc/hosts.deny
    create: yes
    line: 'rpcbind : ALL' 
    state: present
    insertafter: EOF

- name:  Setup NFS - hosts.allow
  lineinfile: 
    dest: /etc/hosts.allow
    create: yes
    line: 'rpcbind : {{ hostvars["master"]["ansible_eth0"]["ipv4"]["address"] }}' 
    state: present
    insertafter: EOF

- name:  Mount NFS
  mount: name={{ item.name }} state=mounted src={{ item.src }} fstype=nfs4
  with_items:
     - { name: '/home', src: '{{ hostvars["master"]["ansible_eth0"]["ipv4"]["address"] }}:/home' }
     - { name: '/mnt/home', src: '{{ hostvars["master"]["ansible_eth0"]["ipv4"]["address"] }}:/mnt/home' }
     - { name: '/mnt/datastore', src: '{{ hostvars["master"]["ansible_eth0"]["ipv4"]["address"] }}:/mnt/datastore' }

- name:  Template init script
  template: src=hadoop-init-worker.j2 dest=/etc/init.d/hadoop-init
    
- name:  Set up init script permissions
  file: path=/etc/init.d/hadoop-init owner=root group=root mode=0755

- name:  Setup init script for boot
  service: name=hadoop-init enabled=yes